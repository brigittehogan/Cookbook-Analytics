{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "- Brigitte Hogan (bwh5v@virginia.edu) & Jason Tiezzi (jbt5am@virginia.edu)  \n",
    "- DS 5001: Exploratory Text Analytics  \n",
    "- April 2020  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = gray>\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook\n",
    "\n",
    "1. Creates a reduced `TFIDF` table; that is, select only the top 5,000 most significant terms.\n",
    "\n",
    "2. Performs PCA on the reduced `TFIDF` \"by hand,\" i.e. create a covariance matrix of features, apply eigen-decomposition, select components, etc. In the process, generate `COMPS`, `LOADINGS`, and `DCM` tables from your results (as in the in-class example).\n",
    "\n",
    "3. Using whatever visualization libraries you can*, inspect the first three components and answer the following questions:\n",
    "\n",
    "    (1) What `LIB` feature (author or genre) does the first principal component (PC) separate?\n",
    "\n",
    "    (2) Based on the first PC, what two novelists are most opposite to (distant from) each other?\n",
    "\n",
    "    (3) Based on the second PC, what two novelists are most opposite to each other?\n",
    "\n",
    "    (4) Based on the third PC, what two novelists are most opposite to each other?\n",
    "\n",
    "    (5) Based on your knowledge of linguistic annotations, what implicit feature do you think accounts for the clear separation of novels in our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../Notebooks/Homework/Homework07_bwh5v/HW_7_DATA/'\n",
    "\n",
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num'] # define OHCO\n",
    "CHAPS = ['book', 'chapter'] # alternate OHCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import norm\n",
    "import plotly_express as px\n",
    "import seaborn as sns\n",
    "from scipy.linalg import eigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def tfidf (token, ohco, bag='CHAPS', count_method='n', item_type='term_id', tf_method='sum', idf_method='standard'):\n",
    "    ## Arguments -----------------------------------------------------------------------------------------\n",
    "    # token (pandas dataframe): must have term_str and term_id or stem_porter\n",
    "    # bag (string) = OHCO_level, either - BOOKS, CHAPS, PARAS, SENTS\n",
    "    # count_method (string): either 'n' (default) for n tokens/ regular or 'c' for distinct tokens/ binary\n",
    "    # item_type (string): type of item to count, either 'term' for terms or 'stem' for stems\n",
    "    # tf_method (string): tf method - sum (default), max, log, double_norm, raw, binary\n",
    "    # idf_method (string): idf method - standard (default), max, or smooth\n",
    "    \n",
    "    ## Create OHCO Dictionary for Bag --------------------------------------------------------------------\n",
    "    #OHCOdict = {\n",
    "    #    \"BOOKS\": ['book_id'],\n",
    "    #    \"CHAPS\": ['book_id', 'chap_num'],\n",
    "    #    \"PARAS\": ['book_id', 'chap_num', 'para_num'],\n",
    "    #    \"SENTS\": ['book_id', 'chap_num', 'para_num', 'sent_num']\n",
    "    #    }\n",
    "    OHCOdict = {\n",
    "        \"BOOKS\": [ohco[0]],\n",
    "        \"CHAPS\": [ohco[0], ohco[1]],\n",
    "        \"PARAS\": [ohco[0], ohco[1], ohco[2]],\n",
    "        \"SENTS\": [ohco[0], ohco[1], ohco[2], ohco[3]]\n",
    "        }\n",
    "    theBag = OHCOdict[bag]\n",
    "    \n",
    "    ## Create Bag-of-Words/Stems -------------------------------------------------------------------------\n",
    "    BOW = token.groupby(theBag + [item_type])[item_type].count().to_frame().rename(columns={item_type:'n'})\n",
    "    \n",
    "    ## Add Binary Count Column ---------------------------------------------------------------------------\n",
    "    BOW['c'] = BOW.n.astype('bool').astype('int')\n",
    "    \n",
    "    ## Create Document Term Frequency Matrix -------------------------------------------------------------\n",
    "    #DTCM = BOW[count_method].unstack().fillna(0)\n",
    "    DTCM = BOW[count_method].unstack(fill_value=0) # Raf's\n",
    "    \n",
    "    ## Compute TF ----------------------------------------------------------------------------------------\n",
    "    if tf_method == 'sum':\n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "    elif tf_method == 'max':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'log':\n",
    "        TF = np.log10(1 + DTCM.T)\n",
    "    elif tf_method == 'raw':\n",
    "        TF = DTCM.T\n",
    "    elif tf_method == 'double_norm':\n",
    "        tf_norm_k = .5\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "        TF = tf_norm_k + (1 - tf_norm_k) * TF[TF > 0]\n",
    "    elif tf_method == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "    \n",
    "    ## Compute IDF ---------------------------------------------------------------------------------------\n",
    "    N = DTCM.shape[0]\n",
    "    DF = DTCM[DTCM > 0].count()   \n",
    "    \n",
    "    if idf_method == 'standard':\n",
    "        IDF = np.log10(N / DF)\n",
    "    elif idf_method == 'max':\n",
    "        IDF = np.log10(DF.max() / DF) \n",
    "    elif idf_method == 'smooth':\n",
    "        IDF = np.log10((1 + N) / (1 + DF)) + 1 \n",
    "\n",
    "    ## Compute TFIDF -------------------------------------------------------------------------------------\n",
    "    TFIDF = TF.T * IDF\n",
    "    \n",
    "    return TFIDF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_tfidf(TOKEN, bag=CHAPS, count_method='n', tf_method='sum', idf_method='standard', item_type='term_id'):\n",
    "    \n",
    "    # Create bag of items (terms or stems)\n",
    "    BOW = TOKEN.groupby(bag + [item_type])[item_type].count().to_frame().rename(columns={item_type:'n'})\n",
    "\n",
    "    # Add binary count column\n",
    "    BOW['c'] = BOW.n.astype('bool').astype('int')\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    DTCM = BOW[count_method].unstack(fill_value=0)#.astype('int')\n",
    "    \n",
    "    # Compute TF\n",
    "    if tf_method == 'sum':\n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "    elif tf_method == 'max':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'log':\n",
    "        TF = np.log10(1 + DTCM.T)\n",
    "    elif tf_method == 'raw':\n",
    "        TF = DTCM.T\n",
    "    elif tf_method == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "        TF = tf_norm_k + (1 - tf_norm_k) * TF[TF > 0] \n",
    "    elif tf_method == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')  \n",
    "    \n",
    "    # Compute IDF\n",
    "    N = DTCM.shape[0]\n",
    "    DF = DTCM[DTCM > 0].count()\n",
    "    if idf_method == 'standard':\n",
    "        IDF = np.log10(N / DF)\n",
    "    elif idf_method == 'max':\n",
    "        IDF = np.log10(DF.max() / DF) \n",
    "    elif idf_method == 'smooth':\n",
    "        IDF = np.log10((1 + N) / (1 + DF)) + 1\n",
    "    \n",
    "    # Compute TF-IDF\n",
    "    TFIDF = TF.T * IDF\n",
    "    return TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vis_pcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def vis_pcs(M, a, b, label='author', prefix='PC'):\n",
    "    fig = px.scatter(M, prefix + str(a), prefix + str(b), \n",
    "                     color=label, \n",
    "                     hover_name='doc', \n",
    "                     marginal_x='box',\n",
    "                     marginal_y='box',\n",
    "                     width=1000, height = 600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB   = pd.read_csv(data_dir + 'novels-LIB.csv')                        # book, genre, author\n",
    "VOCAB = pd.read_csv(data_dir + 'novels-VOCAB.csv').set_index('term_id') # term_id, term_str, tfidf_sum, n\n",
    "TOKEN = pd.read_csv(data_dir + 'novels-TOKENS.csv')                     # OHCO, pos, term_str, term_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = LIB.rename(columns={\"book\": \"book_id\"}).set_index('book_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['genre_full'] = LIB.genre.replace({'d': 'detective', 'g': 'gothic fiction', 'nh': 'unknown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>author</th>\n",
       "      <th>genre_full</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>secretadversary</th>\n",
       "      <td>d</td>\n",
       "      <td>christie</td>\n",
       "      <td>detective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>styles</th>\n",
       "      <td>d</td>\n",
       "      <td>christie</td>\n",
       "      <td>detective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moonstone</th>\n",
       "      <td>d</td>\n",
       "      <td>collins</td>\n",
       "      <td>detective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adventures</th>\n",
       "      <td>d</td>\n",
       "      <td>doyle</td>\n",
       "      <td>detective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baskervilles</th>\n",
       "      <td>d</td>\n",
       "      <td>doyle</td>\n",
       "      <td>detective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                genre    author genre_full\n",
       "book_id                                   \n",
       "secretadversary     d  christie  detective\n",
       "styles              d  christie  detective\n",
       "moonstone           d   collins  detective\n",
       "adventures          d     doyle  detective\n",
       "baskervilles        d     doyle  detective"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = TOKEN.rename(columns={\"book\": \"book_id\",\"chapter\": \"chap_num\"}).set_index(OHCO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create reduced TFIDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "TFIDF1 = tfidf(TOKEN, ohco=OHCO, bag=\"CHAPS\")\n",
    "TFIDF2 = get_tfidf(TOKEN, bag=['book_id', 'chap_num'])\n",
    "print(TFIDF1.equals(TFIDF2)) # True\n",
    "print(VOCAB.tfidf_sum.round(12).equals(TFIDF1.sum().round(12)))\n",
    "print(VOCAB.tfidf_sum.round(12).equals(TFIDF2.sum().round(12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TFIDF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>tfidf_sum</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aback</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abaft</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term_str  tfidf_sum      n\n",
       "term_id                             \n",
       "0                a   0.000000  28533\n",
       "1            aback   0.003732      9\n",
       "2            abaft   0.000876      2\n",
       "3          abandon   0.006993     44\n",
       "4        abandoned   0.010044     68"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Top 5,000 Significant Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using tfidf_sum for significance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = VOCAB.sort_values('tfidf_sum', ascending=False)[0:5000].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = TOKEN[TOKEN.term_id.isin(VOCAB.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TFIDF.loc[:, TFIDF.columns.isin(VOCAB.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 5000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF.shape  # reduced TFIDF, just significant vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process TFIDF Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize doc vector lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TFIDF.apply(lambda x: x / norm(x, 2), 1) # L2 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COV = TFIDF.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COV.iloc[:5,:10].style.background_gradient() # limit this so it doesn't crash your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose the Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time eig_vals, eig_vecs = eigh(COV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert eigen data to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_IDX = COV.index # for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_VEC = pd.DataFrame(eig_vecs, index=TERM_IDX, columns=TERM_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_VAL = pd.DataFrame(eig_vals, index=TERM_IDX, columns=['eig_val'])\n",
    "EIG_VAL.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_VEC.iloc[:5, :10].style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_VAL.iloc[:5] # this is the ranking principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Principal Components\n",
    "\n",
    "Associate each eigenvalue with its corresponding *column* in the eigenvalue matrix by transposing the  `EIG_VEC` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine eigenvalues and eignvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_PAIRS = EIG_VAL.join(EIG_VEC.T) # join into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_PAIRS.head()                    # term_ids ~ components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and Show Explained Variance\n",
    "\n",
    "We might have usd this value to sort our components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_PAIRS['exp_var'] = np.round((EIG_PAIRS.eig_val / EIG_PAIRS.eig_val.sum()) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG_PAIRS.exp_var.sort_values(ascending=False).head().plot.bar(rot=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick Top 3 Components\n",
    "\n",
    "We pick these based on explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPS = EIG_PAIRS.sort_values('exp_var', ascending=False).head(3).reset_index(drop=True)\n",
    "COMPS.index.name = 'comp_id'\n",
    "COMPS.index = [\"PC{}\".format(i) for i in COMPS.index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPS # each term associated with component and weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect terms associated with eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.loc[[int(x) for x in EIG_PAIRS.sort_values('exp_var').head(10).index], 'term_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADINGS = COMPS[TERM_IDX].T\n",
    "LOADINGS.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADINGS.head(20).style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADINGS['term_str'] = LOADINGS.apply(lambda x: VOCAB.loc[int(x.name)].term_str, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_pos = LOADINGS.sort_values('PC0', ascending=True).head(10).term_str.str.cat(sep=' ') # looking at max pos and neg for 1st three components\n",
    "l0_neg = LOADINGS.sort_values('PC0', ascending=False).head(10).term_str.str.cat(sep=' ')\n",
    "l1_pos = LOADINGS.sort_values('PC1', ascending=True).head(10).term_str.str.cat(sep=' ')\n",
    "l1_neg = LOADINGS.sort_values('PC1', ascending=False).head(10).term_str.str.cat(sep=' ')\n",
    "l2_pos = LOADINGS.sort_values('PC2', ascending=True).head(10).term_str.str.cat(sep=' ')\n",
    "l2_neg = LOADINGS.sort_values('PC2', ascending=False).head(10).term_str.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Books PC0+', l0_pos)\n",
    "print('Books PC0-', l0_neg)\n",
    "print('Books PC1+', l1_pos)\n",
    "print('Books PC1-', l1_neg)\n",
    "print('Books PC2+', l2_pos)\n",
    "print('Books PC2-', l2_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Docs onto New Subspace\n",
    "\n",
    "Get Document-Component Matrix (DCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCM = TFIDF.dot(COMPS[TERM_IDX].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCM # each doc/chapter has distribution of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Labels for Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = LIB.reset_index()\n",
    "LIB[\"title\"] = LIB.book_id\n",
    "LIB = LIB.set_index('book_id')\n",
    "LIB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCM = DCM.join(LIB[['author','genre_full','title']], on='book_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCM['doc'] = DCM.apply(lambda x: \"{}-{}-{}\".format(x.author, x.title, x.name[1]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCM.head(10).style.background_gradient() # Note: Components become features for VOCAB and DOC tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pcs(DCM, 0, 1) # by author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vis_pcs(DCM, 0, 1, label='genre_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_pcs(DCM, 0, 1, label='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pcs(DCM, 1, 2) # by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pcs(DCM, 1, 2, label='genre_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_pcs(DCM, 1, 2, label='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC 0 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pcs(DCM, 0, 2) # author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pcs(DCM, 0, 2, label='genre_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_pcs(DCM, 0, 2, label='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results\n",
    "\n",
    "**1. What `LIB` feature (author or genre) does the first principal component (PC) separate?**  \n",
    "The first principal component (PC0) separates primarily on genre. The second principal component (PC1) does a better job of separating author. \n",
    "\n",
    "**2. Based on the first PC (PC0), what two novelists are most opposite to (distant from) each other?**  \n",
    "Radcliffe & Christie\n",
    "\n",
    "**3. Based on the second PC (PC1), what two novelists are most opposite to each other?**  \n",
    "Austen & Christie\n",
    "\n",
    "**4. Based on the third PC (PC2), what two novelists are most opposite to each other?**  \n",
    "Collins & Austen\n",
    "\n",
    "**5. Based on your knowledge of linguistic annotations, what implicit feature do you think accounts for the clear separation of novels in our data?**  \n",
    "By looking at the loadings, it appears the novels are being separated by proper nouns, most of which are the names of the principal characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
